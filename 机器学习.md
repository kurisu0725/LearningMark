## 概念 Terms

#### Sigmoid：

S形函数

#### ReLU（Rectified Linear Unit)

c * max( 0, b + w x1 )

![image-20230413140418055](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230413140418055.png)

两个ReLU函数可以用来表示一个Sigmoid函数

#### Activation Function 激活函数

激活函数：线性函数逼近非线性

如sigmoid, ReLU Function。

#### Deep Learning、Neural Network

![image-20230413144419031](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230413144419031.png)

![image-20230413141245532](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230413141245532.png)

![image-20230904134320390](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230904134320390.png)

#### Overfitting 过拟合

过拟合，在训练数据上表现好，对预测的结果表现不好
![image-20230413142702415](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230413142702415.png)

#### Regularization 正则化

防止Overfitting

![image-20230416140544602](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230416140544602.png)

![image-20230416140715786](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230416140715786.png)

wi 越小，对输入的变化就越不敏感，越平滑。

![image-20230416140808287](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230416140808287.png)

如果输入有噪声，那么越平滑的函数受到的影响就越小。

#### BackPropagation 反向传播

反向传播，加速计算梯度下降。

to compute the gradients efficiently, we use backpropagation.

![image-20230415202118370](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230415202118370.png)

Cn 表示为 输出与 目标结果的 Cost。

![image-20230415202251189](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230415202251189.png)

可以看到计算 Loss 对 W 的 偏导，就等价于计算每个data 与 其 Cost 的偏导和。（即输入的权重）。

![image-20230415202632975](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230415202632975.png)

由链式推导法则，w 对 c 的偏导 可表示为上图，w 对 z 的偏导很好算。

![image-20230415202932303](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230415202932303.png)

显然，z 对 c 的偏导相当复杂。

![image-20230415203626305](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230415203626305.png)



假设 z 经过 sigmoid 激活函数 变成 a，z 对 C 的偏导可表示为 z 对 a 的 偏导 乘上 a 对 C 的偏导，z 对 a 的偏导即为 sigmoid函数的导数。a 对 C 的偏导 又是一个递归的式子。

![image-20230415204256455](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230415204256455.png)

z' 对 C 的偏导就又回到 z 对 C 的偏导相似的式子了， 而前边的结果依赖于后边的计算。所以名为反向传播。

![image-20230415204517237](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230415204517237.png)

#### How to do classfication

![image-20230417145129613](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230417145129613.png)

Loss函数定义为 输入与输出不符合的次数。

找到最佳式子的方法Gradient Descend 不再适用，对于二元分类有Generative Model，通过条件概率计算得出某个x出现的几率 P(x)。为此我们需要训练出4个参数，P(C1), P(C2), P(x | C1), P(x | C2)。

![image-20230417145537832](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230417145537832.png)

如何计算 P(x | C1) ？

![image-20230417150319554](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230417150319554.png)

假设图中训练集出的点是服从高斯分布的，估测出高斯分布的参数μ和sigma。

![image-20230417150720394](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230417150720394.png)

代入高斯分布的公式，得出大致的Function后相当于知道了这个高斯分布的情况，那么预测点的概率就可以算出。

![image-20230417150810775](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230417150810775.png)

那么如何计算μ和sigma呢？

#### Maximum Likelihood

The Gaussian with any mean μ and covariance  matrix sigma can generate these points. 任意参数的高斯分布都可以生成如图的点，但是可能性不同。

定义高斯分布的几率 = 生成单个点几率的乘积。

![image-20230417151303844](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230417151303844.png)

![image-20230417151558298](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230417151558298.png)

解得

![image-20230417151730795](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230417151730795.png)

采用Generative Model的效果并不好。我们可以采用相同 sigma来计算 Likelihood（μ1，μ2，sigma）

![image-20230417152658413](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230417152658413.png)

#### Posterior Probability

![image-20230417153600761](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230417153600761.png)

推导过程：![image-20230417153806402](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230417153806402.png)

![image-20230417153943895](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230417153943895.png)

一般情况假定 sigma1 = sigma2 = sigma

![image-20230417154309758](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230417154309758.png)

最后要求的概率就是 wx + b 的 sigmoid函数。

#### Logistic Regression 逻辑回归

![image-20230418131228620](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230418131228620.png)

计算使得L(w, b)  最大的 w *, b * 等价于 使得 -In L(w, b) 最小的 w *, b *。

![image-20230418131534274](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230418131534274.png)

**Cross entropy**：交叉熵，表示连个distribution的相似程度

第三步：梯度下降求偏导

![image-20230418132130347](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230418132130347.png)

![image-20230418132347892](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230418132347892.png)

![image-20230418132552857](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230418132552857.png)

Logistic 和 Linear Regression 的step3 其实都一样，唯一不一样的是取值。

#### Cross Validation

将Training Data 分为 Training data 和 Validation data

#### 逃离鞍点

![image-20230903201845931](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230903201845931.png)

H矩阵为二阶偏导，

![image-20230903201745872](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230903201745872.png)

![image-20230903202348040](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230903202348040.png)

#### Batch 和 动量 momentum

![image-20230903203903064](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230903203903064.png)

动量：

![image-20230903204339997](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230903204339997.png)

#### 自动调整Learning Rate

##### Adagrad

![image-20230903205440024](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230903205440024.png)

##### RMSProp

![image-20230903205735062](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230903205735062.png)

##### Adam：RMSProp + Momentum



### CNN Convolution Nerual Network 卷积神经网络

#### 共享参数 parameter sharing

每个neruon都检测所对应位置的窗口，但是对于检测的同一特征没有必要都设置完全不同的neruon，权重是可以共享的，区别在于不同窗口位置的输入不同。

![image-20230904130258248](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230904130258248.png)

![image-20230904130615880](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230904130615880.png)

每个窗口都有一组neuron来检测，而不同组neuron共享参数。

换句话说，不同的receptive field neuron共用参数，也就是 一个filter 扫过一遍 input image.

### Spatial Transformer Layer

CNN 对图像scaling 和 rotation的效果并不好，需要数据增强。

![image-20230904140727853](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230904140727853.png)

Spatial Transformer Layer可以处理图像的缩放和旋转。

![image-20230904143047399](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230904143047399.png)

该Layer 计算得出 当前Layer的目标 和 上一层Layer之间的关系，也就是仿射变换，得到坐标变换使用的矩阵。

![image-20230904143509259](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230904143509259.png)

由于仿射变换的结果坐标可能带小数，需要用到interpolation差值来计算结果的值。

### 吴恩达

#### Feathue Scaling 特征缩放 

如果特征在不同的尺度下，不同的权重更新的速率会差异很大，不利于算法的收敛。

| Mix-max normalization      | $$ z = \frac{x-min(x)}{max(x) - min(x)}$$                    |
| -------------------------- | ------------------------------------------------------------ |
| **Constant normalization** | ![image-20230719202806329](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230719202806329.png) |
| **Mean normalization**     | ![image-20230719202836041](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230719202836041.png) |
| **Z-score normalization**  | $$ z = \frac{x - \mu}{\sigma}$$                              |

##### Standard Deviation 标准差

标准差也被称为标准偏差，或者实验标准差，在概率统计中最常使用作为统计分布程度上的测量依据。

标准差 $$ \sigma = \sqrt{\frac{\sum_{i=1}^{n}(x_i-\mu)^2} {n} }$$

其中 $$\mu = \frac{\sum_{i = 1}^{n}x_i }{n}$$ ,为平均值。

在Z-score normalization中，

$$ z = \frac{x - \mu}{\sigma}$$

对于每个特征 $$x_i$$ 计算对应的 均值 $$\mu_i$$ 和 标准差 $$ \sigma_i$$ ,得到标准化后的数据。

##### 检查梯度下降是否收敛

 ##### 学习率的选择 Choosing the Learning Rate

| ![image-20230719211237572](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230719211237572.png) | 程序出错 or 学习率太大     |
| ------------------------------------------------------------ | -------------------------- |
| ![image-20230719211431697](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230719211431697.png) | **程序出错 or 学习率太大** |

检查梯度下降是否正确：可以使用足够小的学习率，来检查损失函数是否每次迭代都减少。

#### Logistic Regression 逻辑回归

##### Sgmoid function

sigmoid函数:	$$ g(z) = \frac{1}{1+e^{-z}} ,0 < g(z) < 1$$



![image-20230720213710645](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230720213710645.png)



##### Decision boundary 决策边界

在逻辑回归中，我们令 $$ z = w * x + b $$，则

$$ f_w,_b(x) = g(z) = g(w*x + b)$$ ，通常对于预测的结果

 $$ if \quad f_w,_b(x) >= 0.5, 我们认为\quad\widehat{y} = 1 $$

反之 $$ if \quad f_w,_b(x) < 0.5, 我们认为\quad\widehat{y} = 0 $$。

其中 $$ f_w,_b(x) = g(z) = 0 $$ 的边界值称为决策边界。

**Tips：**如果使用的特征 x 都是一维的，那么决策边界也将是一条直线。想得到曲线的边界，需要将特征定义成高的幂次。

##### 逻辑回归的 Cost Function 

代价函数Cost Function $$ J(\vec{w},b) = \frac{1}{m} \sum_{i=1}^{m} L(f_\vec{w},_b(\vec{x}^{(i)}), y^{(i)})$$

而单个样本计算出的结果称为**损失函数（LossFunction）**
$$
L(f_\vec{w},_b(\vec{x}^{(i)}), y^{(i)}) = 
\left\{
		\begin{array}{rcl}
		-log(f_\vec{w},_b(\vec{x}^{(i)})),	&{if\quad y^{(i)} == 1} \\
		-log(1-f_\vec{w},_b(\vec{x}^{(i)})),	&{if\quad y^{(i)} == 0}\\
		\end{array}
\right.
$$

##### Regularization 正则化

为防止Overfitting, 但是保留所有的features，将一些系数w尽可能的调小来让函数依然能拟合。

![image-20230721154030404](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230721154030404.png)

###### 正则化代价函数

$$ J(\vec{w},b) = \frac{1}{2m}\sum_{i=1}^{m}(f_\vec{w},_b(\vec{x}^{(i)}) - y^{(i)})^{2} + \frac{\lambda}{2m}\sum_{i=1}^{m}w_j^{2}$$

一般在正则化Cost Function时，只对w参数进行正则化，即将w设置为接近0的值，此时 $$J(\vec{w},b)$$ 左边的式子表示 fit data的程度，而 正则化参数(regularization parameter)$$\lambda $$ 来控制正则化的程度来防止过拟合。

### Deep Learning 深度学习

#### Activation Function 激活函数

- Linear activation function

  ![image-20230727223425141](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230727223425141.png)

- Sigmoid 

  ![image-20230727223442219](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230727223442219.png)

- ReLU

  ![image-20230727223454564](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230727223454564.png)

#### Multiclass Classfication 多类分类问题



#### Mult-label Classfication 多标签分类（多个输出）

#### Adam Algorithm 

不同于梯度下降

#### Convolutional Layer 卷积层

### Evaluating a model 模型评估

#### Cross Validation 交叉验证

数据集分为training set 和 test set，为了便利地评估模型好坏，将training set 分割一部分称为 cross validation set。

training set 用于训练，然后validation set 用于评估训练的模型好坏，完成后再对test set 进行预测。

#### High variance and High bias 偏差与方差

### Adding data

#### Data Augmentation 数据增强

![image-20230801192550057](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230801192550057.png)

通过引入distortions，比如识别A字符时，仅仅只将A字符平移、旋转、镜像、扭曲等等得到新的training sample。

#### Data Synthesis 数据合成

#### Transfering Learning 迁移学习

using data from a different task.



### 类不平衡问题

![image-20230801195948333](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230801195948333.png)

当一种罕见病发病率为0.5%时，此时就算一直打印```y=0``` 也有高达99.5%的正确率。

#### Precision/Recall

![image-20230801200626442](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230801200626442.png)

Precision(准确率）：TP / ( TP + FP )

Recall(回归率):  TP / (TP + FN)

#### F1 Score

$$
F1 Score = \frac{1}{\frac{1}{2}(\frac{1}{P} + \frac{1}{R} )},  P=Precision,R = Recall
$$

调和平均数，

### Decision Tree 决策树

**Prob1.如何选择决策树节点的特征？**

**Prob2.什么时候停止splitting?**

![image-20230801203842399](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230801203842399.png)



#### Maximize Purity 

在选择决策树节点的特征时，如何划分数据集？尽可能的让划分的集合拥有共同的特征，也就是纯度尽可能的高。

熵（entropy）的定义：a measure of  the impurity of a set of data.
$$
H(A) = -\sum_{j=1}^{k}p_jlog_2(p_j)
$$


###### **Information Gain 信息增益**

当节点被划分为两个分支后，每个分支最后拥有的数据集大小可能会有差距，也就是说单单计算实际的熵并不能准确反应结果。需要加上权重。

![image-20230802193836185](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230802193836185.png)

如上图1，知道分支后的结果为 
$$
\frac{5}{10}H(0.8)+\frac{5}{10}H(0.2)
$$
而对于选择特征这一步操作，有关联的是熵的减少(Reduction of entropy)，即选择这一特征相对上一节点能带来多少熵的减少。

如果把父节点定义为$$p1^{root}$$ , 左儿子为 $$p1^{left}$$,右儿子为$$p1^{right}$$ ,那么信息增益的定义如下：
$$
Information Gain = H(p1^{root}) - (w^{left}H(p1^{left})) + w^{right}H(p1^{right}))
$$
**Tips：** 如果熵的减少太少，你可以决定某一阈值是否继续构造决策树来防止过拟合。



#### One-Hot Encoding

If a categoriacal feature can take on k values, create k binary features( 0 or 1 valued).

#### Regression Tree 回归树

对于离散的值我们都可以用0、1表示，那么对于连续值应该如何处理。

![image-20230802200122503](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230802200122503.png)

引入weight特征，特征值如上图。

在信息增益中，我们计算的熵H(p1)是特征值是离散值的情况下的。若特征值是连续值那就不适用于信息熵的计算了，因为不能简单的划分为是否属于某一类了。

这时候将熵的计算替换为计算集合的方差**Variance**，即可。
$$
Information Gain = V(p_1^{root}) - (w^{left}V(p1^{left}) + w^{right}V(p1^{right}))
$$
此时即可解决Regression Problem，在决策树的叶节点的处理：以weight为例，计算叶节点的数据集合的平均weight。

#### 使用多个决策树

由于单个决策树对数据的变动十分敏感，当改动一部分时，就可能引起节点选择特征的信息增益变化，导致划分的集合不同，最终结果大不相同。

#### Tree Ensemble

#### Sampling with replacement  放回抽样

我们通过对同一训练集进行抽样放回来 构造出新的训练集——>生成新的决策树——>得到森林。

#### Random Forest Algorithm 随机森林

**Randomizing the feature choice**

在每一个节点上，选择特征时，假设有n个特征，那么随机选择k个特征的子集（k < n)，让算法在子集中选择特征。

通常选择 $$k = \sqrt{n}$$

#### XGBoost

与随机森林不同的是，在第二次及之后生成决策树时的随机取样的策略。

算法将重点放在预测出错的样例上，让misclassify的样例有更高的概率被抽样到。

![image-20230804164219044](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230804164219044.png)

#### When to use decision trees/neural network

当数据是有结构的（structed）的，适合使用决策树。对于图像、音频、视频这些则不太好用。

neural network对于所有类型的数据都比较通用，缺点是训练数据比较多，训练速度慢。

### Unsupervised Learning 无监督学习

**Unsupervised Leanring** 用于 **Clustering （聚类）**和 **Anomaly detection（异常检测）**。

#### Clustering Algorithm:

#### K-means 

- [ ] Randomly initialize K cluster centroids: $$\mu_1,\mu_2,...,\mu_k$$

- [ ] Repeat{

  ​	#Assign points to cluster centroids

  ​	for i  = 1 to m

  ​			$$c^{(i)}$$ := index(from 1 to K) of  cluster centroid 

  ​						closest to $$x^{(i)}$$

  ​			($$c^{(i)} = min_k ||x^{(i)} - \mu_k||^{2}$$)

  ​	#Move cluster centroids

  ​	for k = 1 to K

  ​				$$\mu_k$$ := average (mean) of points assigned to cluster k

  }

##### **Cost Function**:

**notation:**

$$c^{(i)}$$ = index of cluster (1, 2, ..., K) to which example $$x^{(i)}$$ is currently assigned

$$\mu_k$$ = cluster centroid k

$$\mu_c(i)$$ = cluster centroid of cluster to which example $$x^{(i)}$$ has been 

那么K-means算法的Cost Function表示为：
$$
J(c^{(1)},...,c^{(m)},\mu_1,...,\mu_k) = \frac{1}{m}\sum_{i=1}^{m}\|x^{(i)} - \mu_c(i)\|^{2}
$$
也就是说 Loss函数的即是 每个样例与其所属的 cluster centroid 距离的 开平方。

##### **Random initialization** 

Choos K < m, randomly pick K training examples.

Set $$\mu_1,\mu_2,...,\mu_k$$ equal to these K examples.

##### Choosing the Number of Clusters

###### **Elbow method**

![image-20230804215113610](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230804215113610.png)

### Anomaly Detection 异常检测

#### Gauss Distribution 高斯分布

![image-20230804220938794](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230804220938794.png)



公式如下：
$$
p(x) = \frac{1}{\sqrt{2\pi}\sigma} e^{\frac{-(x-\mu)^{2}}{2\sigma^{2}}}
$$
设置 
$$
\mu = \frac{1}{m}\sum_{i=1}^{m}x^{(i)}
$$

$$
\sigma^{2} = \frac{1}{m} \sum_{i=1}^{m} (x^{(i)} - \mu) ^ {2}
$$

对于每个features来说：
$$
\mu_j = \frac{1}{m}\sum_{i=1}^{m}x_j^{(i)}
$$

$$
\sigma_j^{2} = \frac{1}{m} \sum_{i=1}^{m} (x_j^{(i)} - \mu_j) ^ {2}
$$

**Algorithm**:

1. 选择 你认为最有可能

2. 计算出 每个特征对应分布的 $$\mu_j, \sigma_j^{2}$$

3. 对于 新的样例 x, 计算 P(x):
   $$
   p(x) = \prod_{j=1}^{n}p(x_j;\mu_j;\sigma_j^{2}) = \\
   	\prod_{j=1}^{n}\frac{1}{\sqrt{2\pi}\sigma_j} e^{\frac{-(x_j-\mu_j)^{2}}{2\sigma_j^{2}}}
   $$

4. 检查 $$ if \quad p(x) < \varepsilon$$ 

#### Evaluation 如何评估模型

True positive, True negative, False Positive, False Negative

Precision\Recall

F1 Score.

#### Anomaly Detection vs. Supervised Learning

当你拥有标签为y=0和y=1的样本时，如何选择是使用Anomaly Detection还是Supervised Learning？

y=1为positive example，即为发生异常的样例。

**Anomaly Detection:**

适用于拥有少量的positive examples (y = 1), 大量的negative examples (y = 0)。

并且未来发生的异常可能是样本中未出现过的。因为异常检测是通过negative examples得出的分布模型，任何有偏差deviation的样例都会被认为是anomaly，包括未出现过的异常。

**Supervised Learning：**

在监督学习中，我们倾向于认为positive和negative example 数量相当，并且异常很有可能来自于训练集。

#### Choosing Features

对于某些Non-gaussian features，可以绘制出直方图来观察是否符合高斯分布的形状，不符合则将特征变换成近似的形状。

如
$$
x_1 = log(x_1 + c) \\
x_2 = \sqrt{(x_2)}
$$
等等的变换。

### Recommender System 推荐系统

下面是一个用户对电影的评价。

![image-20230805204152639](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230805204152639.png)

#### Cost Function to learn w and b

![image-20230805203708500](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230805203708500.png)
$$
J(w^{(j)}, b^{(j)}) = \frac{1}{2m^{(j)}} \sum_{i:r(i,j)=1} (w^{(j)}*x^{(i)} + b^{(i)} - y^{(i,j)})^{2}
$$
对于每一位**user j**来说，要计算他的代价，$$w^{(j)}*x^{(i)}+b^{(i)}$$也就是他评价过的第 **i** 个电影的分数。

加入正则化**Regularization**:
$$
\underset{w^{(j)},b^{(j)}}{min}J(w^{(j)}, b^{(j)}) = \frac{1}{2m^{(j)}} \sum_{i:r(i,j)=1} (w^{(j)}*x^{(i)} + b^{(i)} - y^{(i,j)})^{2} \\+ \frac{\lambda}{2m^{(j)}}\sum_{k=1}^{n}(w_k^{(j)})^{2}
$$
其中n = number of features.

$$To\quad learn\:w^{(1)},b^{(1)},w^{(2)},b^{(2)},...,w^{(n_u)},b^{(n_u)}:$$
$$
\underset{w^{(1)},b^{(1)},...,w^{(n_u)},b^{(n_u)}} {min} J(w^{(1)},b^{(1)},...,w^{(n_u)})  = \\ \frac{1}{2}\sum_{j=1}^{n_u} \sum_{i:r(i,j)=1} (w^{(j)}*x^{(i)} + b^{(i)} - y^{(i,j)})^{2} \\+ \frac{\lambda}{2}\sum_{j=1}^{n_u} \sum_{k=1}^{n}(w_k^{(j)})^{2} \tag{1}
$$


#### Collaborative Filtering 协同过滤算法 

在上面的电影评价系统中，标注出了x1,x2两个特征来表示romance,action电影的类型。如果事先并没有这些特征值，又该怎么得出来呢？

![image-20230805205545478](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230805205545478.png)

**Cost function to learn feature's value x**

$$Given\quad w^{(1)}, b^{(1)}, w^{(2)},b^{(2)},...,w^{(n_u)},b^{(n_u)},  (n_u = number\, of\, users)$$

$$To\:learn\:x^{(i)}:$$
$$
J(x^{(i)}) = \frac{1}{2}\sum_{j:r(i,j)=1}(w^{(j)}*x^{(i)}+b^{(j)}- y^{(i,j)})^{2} + \\
	\frac{\lambda}{2}\sum_{k=1}^{n}(x_k^{(i)})^{2}
$$
$$To\:learn\:x^{(1)},x^{(2)},...,x^{(n_m)}:\quad (n_m = number\:of\:movies)$$
$$
\underset{x^{(1)},...,x^{(n_m)}}{min}J(x^{(1)},...,x^{(n_m)}) = \\ \frac{1}{2}\sum_{i=1}^{n_m}\sum_{j:r(i,j)=1}(w^{(j)}*x^{(i)}+b^{(j)}- y^{(i,j)})^{2} + \\
	\frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(i)})^{2} \tag{2}
$$
观察公式（1）、（2）也即计算 w,b 和 x 的 Cost Function 可以发现 他们的第一项计算 其实是一样的，将公式相加得到：
$$
J(w,b,x) = \frac{1}{2} \sum_{(i,j):r(i,j)=1}(w^{(j)} \cdot x^{(i)} + b^{(j)} - y^{(i,j)})^{2} + \\
\frac{\lambda}{2}\sum_{j=1}^{n_u} \sum_{k=1}^{n}(w_k^{(j)})^{2} + \frac{\lambda}{2}\sum_{i=1}^{n_m}\sum_{k=1}^{n}(x_k^{(i)})^{2} 
$$
下一步就是要求出所有的 $w,b,x$ 使得 Cost Function最小。
$$
\underset{w,b,x}{min}J(w,b,x)
$$
与线性回归**Linear  Regression**不同的是，这里我们不只w,b是未知，特征x也是未知，所有一共有3种未知数。

接下来进行**Gradient Descent**

![image-20230805213403727](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230805213403727.png)

##### Binary Labels

通常这类标签是Like or not like,。

![image-20230805214653503](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230805214653503.png)

![image-20230805214956500](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230805214956500.png)

#### Mean Normalization 均值归一化

![image-20230805220050885](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230805220050885.png)

将User和Movie数据看成矩阵，得到：

**Mean Normalization Implement with TensorFlow：**

![image-20230805221357461](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230805221357461.png)

#### Content-based Filtering 基于内容的过滤

**Collaborative filtering vs Content-based filtering**

| **Collaborative filtering** | 推荐items基于 其他用户的评分      |
| --------------------------- | --------------------------------- |
| **Content-based filtering** | **推荐items基于用户和item的特征** |

**Content-based Filtering** 需要定义用户和item的特征。

![image-20230806161618241](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230806161618241.png)

##### Deep Learning for Content-based Filtering

![image-20230806163458785](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230806163458785.png)

$X_u,X_m$，user的特征向量和Movie的特征向量通过神经网络得到两个同大小的向量$V_u,V_m$ ，两者的dot-product 构成输出。

**Cost Function：**
$$
J=\sum_{(i,j):r(i,j)=1}(v_u^{(j)} \cdot v_m^{(i)} - y^{(i,j)})^{2} \\ + NN\: regularization\:term
$$

##### 从大目录中推荐

**Retrieval % Ranking**



### Question

$$

$$



#### Q1 Why we want "Deep" network, not "Fat" network?

为什么要加那么多层的hidden layer，用足够多的ReLU或Sigmoid函数不是也可以近似表示曲线么？

#### Q2 Logistic Regression 为什么不能用 Square Error 来定义函数的好坏？

![image-20230418133144348](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230418133144348.png)

若输出为0，目标为1时，输出与目标差距很大，但此时偏导为0，变量wi 不移动。

![image-20230418133340998](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20230418133340998.png)

使用Cross Entropy离目标越远，移动越快，而Square Error很平坦，不容易得到好的结果。

#### Q3. 分类和聚类的差别

